import numpy as np
import assets as at
import portfolio as pf
import data_loader as dl
import matplotlib
import matplotlib.pyplot as plt
import copy
import pandas as pd
matplotlib.use('MacOSX')
# -*- coding: utf-8 -*-
"""markowitz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UwkVrmi3nAlzoCCwefTlbF9dSTnC2fAE

# Markowitz Efficient Portfolio - Python
Segue notebook python3 per il calcolo dei parametri della frontiera efficiente.

Gli input (i ritorni mensili dei simboli RICs indicati nel codice) sono stati scaricati da Eikon tramite API Gateway locale, poi salvati in formato .json e predisposti nella cartella in cui il notebook gira. Il periodo di analisi dipende dal file ed il dato è contenuto nel file stesso.

Per prima cosa, effettuiamo il setup del kernel corrente (in modo artigianale, per comodità, con sys).
"""

# setup jupiter kernel with requirements
# import sys
# !{sys.executable} -m pip install numpy scipy eikon matplotlib cvxopt

"""Una volta predisposto l'ambiente, carichiamo i dati precedentemente scaricati da Eikon ed organizzati in file .json. Sono presenti quattro diversi scenari:


1.   Scenario 'assets-test.json': file utilizzato ai fini del testing del codice, 3 soli simboli, 6 soli periodi mensili, date corrette ma fittizie. Questo file è utile solo ai fini del testing del codice.
2.   Scenario 'assets-small.json': file di piccole dimensioni (periodo semestrale, simboli provenienti da DAX30, CAC40 ed IBEX 30, frequenza mensile)
3.   Scenario 'assets-large.json': file di medie dimensioni (periodo decennale, simboli provenienti da DAX30, CAC40 ed IBEX 30, frequenza mensile)
4.   Scenario 'assets-production.json': file di grandi dimensioni (periodo ventennale, simboli provenienti da DAX30, CAC40 ed IBEX 30, frequenza mensile)

Nel codice sono presenti funzionalità dedicate alla creazione e manipolazione di ulteriori insiemi di simboli e relativi performance temporali.
"""

# load data from file
ek_data_processed = dl.load_data_from_file('assets-test.json')
target_assets = ek_data_processed['target_assets']
print("Assets list: ")
print(target_assets)
print("Date from: " + ek_data_processed["start_date"])
print("Date to: " + ek_data_processed["end_date"])
print("Number of assets: " + str(len(target_assets)))
print("Number of time samples: " + str(len(ek_data_processed[target_assets[0]])))

"""## Assets
Vengono qui caricati il RIC dell'asset e la relativa serie temporale di ritorni menili %.
```
assets.add_or_modify_asset(asset_name, ek_data_processed[asset_name])
```
Una volta caricati tutti gli asset viene calcolata la matrice di covarianza.
```
assets.compute_and_set_covariance_matrix()
```
Ogni portafoglio farà riferimento a questa entità per disporre della matrice di correlazione, che viene così calcolata una sola volta.
"""

# load data into Assets, and compute covariance matrix
assets = at.Assets()
for asset_name in target_assets:
    assets.add_or_modify_asset(asset_name, ek_data_processed[asset_name])
assets.compute_and_set_covariance_matrix()
print('Covariance Matrix')
print(assets.get_covariance_matrix())
print('Covariance Matrix shape')
print(assets.get_covariance_matrix().shape)

"""## Simulazione di N portafogli
Si creano qui 50000 portafolgi costruiti randomizzando e normalizzando i pesi del vettore `weights`. Si costruisce lo scatter plot volatilità vs rendimento atteso. Si osserva che più ampio è l'insieme degli strumenti analizzati, maggiore tende ad essere la distanza tra la frontiera efficiente ed il pool di portafogli simulati. Questo si può spiegare osservando che il vettore dei pesi viene randomizzato intorno ad un vettore medio in cui ogni componente è centrata intorno a 0.5. Il vettore efficiente, nei vari casi, presenta invece pochi valori estremi (e più specificamente: ha la maggior parte delle componenti ''piccole'' e poche componenti a valori ''grandi'').
"""

# generate 50000 random portfolios
n_weights = len(target_assets)
portfolios = list()
# np.random.seed(0) # let's make it always the same
for i in range(0, 50000):
    weights = np.random.rand(n_weights)
    weights /= np.sum(weights)
    risk_free_rate = 0.001
    portfolios.append(pf.Portfolio(assets, weights, risk_free_rate))

print("Number of portfolios: " + str(len(portfolios)))
print("Number of weights: " + str(len(target_assets)))

# prepare data for plotting
x = [p.get_expected_volatility() for p in portfolios]
y = [p.get_expected_return() for p in portfolios]

print("Number of data points, x: " + str(len(x)) + " / y: " + str(len(y)))

# Commented out IPython magic to ensure Python compatibility.
# plot data
# %matplotlib inline
plt.rcParams['figure.figsize'] = [15, 10]
plt.xlabel("expected volatility")
plt.ylabel("expected return");
plt.scatter(x, y);

"""## Portafoglio ottimo
Per il calcolo del portfoglio ottimo si deve minimizzare la quantità `w * C * w'` sotto il vincolo della somma dei pesi del vettore `w` pari a 1. Il problema di ottimizzazione è quadratico, e viene quindi utilizzato un solver non lineare (`qp` dal pacchetto `cvxopt`). É possibile ri-formulare il problema togliendo il vincolo di non-negatività, in questa versione il codice implementa detto vincolo (no shorting). Credits a https://plotly.com/python/v3/ipython-notebooks/markowitz-portfolio-optimization/ per la traduzione delle matrici del problema nel formato `qp`.
"""

# compute efficient portofolios frontier
returns_matrix = assets.get_total_returns()
covariance_matrix = assets.get_covariance_matrix()
target_returns = [x for x in np.arange(0.01, 0.28, 0.025)]
opt_weights, opt_returns, opt_risks = pf.optimal_portfolio_frontier_correct(assets.get_all_assets_average_return(),
                                                                            covariance_matrix, target_returns)

# # plot efficient portfolios frontier
# plt.scatter(x, y);
# plt.plot(opt_risks, opt_returns, 'y-o')
# plt.xlabel("expected volatility")
# plt.ylabel("expected return");
#
# # plot weights
# lines = plt.plot(opt_weights)
# i = 0
# for l in lines:
#     l.set_label(target_assets[i])
#     i += 1
# plt.ylabel("weight (a different color per asset)")
# plt.xlabel("portfolio id");
# plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), ncol=8, mode="expand", borderaxespad=0.)
# plt.show()
#
# # better viz (WIP) (stacked bars)
# plt.figure()
# portfolio_index = [x for x in range(len(opt_weights))]
# portfolio_component_index = [x for x in range(len(opt_weights[0]))]
# bar_components = list()
# for portfolio_component_idx in portfolio_component_index:
#     bar_component = list()
#     for portfolio_idx in portfolio_index:
#         bar_component.append(abs(opt_weights[portfolio_idx][portfolio_component_idx]))
#     bar_components.append(bar_component)
#
# a_dict = dict()
# idx = 0
# for bar_component in bar_components:
#     key_name = target_assets[idx]
#     idx += 1
#     a_dict[key_name] = copy.deepcopy(bar_component)
#
# plt.figure()
# df = pd.DataFrame(a_dict)
# df.astype(df.dtypes).plot.bar(stacked=True)
# plt.show()

# # for ow in opt_weights:
# #  print(np.sum(ow))
# # print(opt_weights)
# # print(bar_components)
# print(bar_components)
# for bar_component in bar_components:
#     plt.bar(portfolio_index, copy.deepcopy(bar_component))
#     plt.show()

# """Spunti per concludere:
# 1. calcolo della CAL a partire da rf e Tangency portfolio, plotting
# 2. verifica che combinazione tangency + global minimum variance portfolio coincida con frontiera efficiente
# """
# # tangency portfolio, plot lines
# risk_free_rate = 0.01
# historical_returns = assets.get_all_assets_average_return()
# t_pf_w = pf.compute_tangency_portfolio(historical_returns, covariance_matrix, risk_free_rate)
# pf.plot_pf_lines(t_pf_w, target_assets)
#
# # tangency portfolio, plot stackbar
# t_pf_w_l = list()
# t_pf_w_l.append(t_pf_w)
# pf.plot_pf_stacked(t_pf_w_l, target_assets)
#
# plt.figure()
# plt.plot(opt_risks, opt_returns, 'y-o')
# plt.xlabel("expected volatility")
# plt.ylabel("expected return")
# t_pf = pf.Portfolio(assets, np.array(t_pf_w), risk_free_rate)
# x_tangent = t_pf.get_expected_volatility()
# y_tangent = t_pf.get_expected_return()
# plt.scatter(x_tangent, y_tangent)

risk_free_rate = 0.01
opt_sharpe = list()
idx = 0
for opt_risk in opt_risks:
    idx += 1
    opt_sharpe.append((opt_weights[idx]-risk_free_rate)/opt_risk)
